{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f405ea66",
   "metadata": {},
   "source": [
    "# DeepReaction Model Prediction\n",
    "\n",
    "This notebook uses a pre-trained DeepReaction model checkpoint to make predictions on a dataset specified by a CSV file and corresponding XYZ files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3d9b4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8534887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "from deepreaction.data.PygReaction import ReactionXYZDataset \n",
    "from deepreaction.module.pl_wrap import Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc73e40",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Modify the parameters below to match your dataset, model checkpoint, and desired output locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d630fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory set to: ./predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_root = './dataset/DATASET_DA_F' \n",
    "dataset_csv = './dataset/DATASET_DA_F/dataset_xtb_final.csv'\n",
    "\n",
    "input_features = ['G(TS)_xtb', 'DrG_xtb'] # Example: ['G(TS)_xtb', 'DrG_xtb'] or [] \n",
    "file_patterns = ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz'] # Patterns to find geometry files\n",
    "id_field = 'ID'            # Column name in CSV for unique identifier\n",
    "dir_field = 'R_dir'       # Column name in CSV for subdirectory containing XYZ files\n",
    "reaction_field = 'reaction' # Column name in CSV for reaction SMILES/identifier (optional, for output)\n",
    "\n",
    "checkpoint_path = './results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4343.ckpt' \n",
    "\n",
    "# Output parameters (Adjust paths as needed)\n",
    "output_csv = './predictions.csv' # Path to save the predictions in CSV format\n",
    "output_dir = './predictions'   # Directory to save prediction outputs (e.g., numpy arrays)\n",
    "\n",
    "# Inference parameters\n",
    "batch_size = 32          # Number of samples per batch for prediction\n",
    "use_cuda = True          # Set to True to use GPU if available, False to force CPU\n",
    "gpu_id = 0               # GPU ID to use if use_cuda is True and GPU is available\n",
    "num_workers = 4          # Number of workers for data loading (set to 0 on Windows if issues arise)\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "# Handle case where input_features might be None\n",
    "if input_features is None:\n",
    "    input_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67a911",
   "metadata": {},
   "source": [
    "## 3. Setup Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db731dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "if use_cuda and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    use_cuda = False # Ensure flag reflects actual device used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab970a57",
   "metadata": {},
   "source": [
    "## 4. Load Model and Extract Target Fields\n",
    "\n",
    "Load the trained model from the checkpoint. The target field names and scaler information (if saved) will be extracted from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec4a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: ./results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4343.ckpt\n",
      "Successfully loaded model. Using target fields from model: ['G(TS)', 'DrG']\n",
      "Model contains scaler information for 2 targets.\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the checkpoint\n",
    "print(f\"Loading model from checkpoint: {checkpoint_path}\")\n",
    "model = None\n",
    "target_fields = None\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"Error: Checkpoint file not found at '{checkpoint_path}'\")\n",
    "    print(\"Please verify the 'checkpoint_path' in the configuration cell.\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure the Estimator class is available from imports\n",
    "        # Use map_location to ensure the model loads correctly whether on CPU or GPU\n",
    "        model = Estimator.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "        model = model.to(device) # Move model to the selected device\n",
    "        model.eval() # Set model to evaluation mode (important!)\n",
    "\n",
    "        # Extract target field names from the loaded model\n",
    "        if hasattr(model, 'target_field_names'):\n",
    "            target_fields = model.target_field_names\n",
    "            print(f\"Successfully loaded model. Using target fields from model: {target_fields}\")\n",
    "        else:\n",
    "             print(\"Warning: Could not automatically determine target fields from the model.\")\n",
    "             # Manually define target_fields here if necessary, e.g.:\n",
    "             # target_fields = ['G(TS)', 'DrG'] \n",
    "             # Make sure this matches the order of outputs from your model!\n",
    "             raise ValueError(\"target_field_names attribute not found in the loaded model.\")\n",
    "            \n",
    "        # Check for scaler information (used for inverse transforming predictions)\n",
    "        if hasattr(model, 'scaler') and model.scaler is not None:\n",
    "             print(f\"Model contains scaler information for {len(model.scaler)} targets.\")\n",
    "        else:\n",
    "             print(\"Warning: Model does not contain scaler information. Predictions will be in scaled units.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from checkpoint: {e}\")\n",
    "        print(\"Ensure the checkpoint file is valid and compatible with the current deepreaction code.\")\n",
    "        model = None # Ensure model is None if loading failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea49871",
   "metadata": {},
   "source": [
    "## 5. Load Dataset for Inference\n",
    "\n",
    "Load the dataset using `ReactionXYZDataset`. This class reads the CSV and finds the corresponding XYZ files based on the provided patterns and directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03007620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./dataset/DATASET_DA_F using CSV ./dataset/DATASET_DA_F/dataset_xtb_final.csv\n",
      "Error checking saved data: 'NoneType' object is not subscriptable\n",
      "Using target fields: ['G(TS)', 'DrG']\n",
      "Using input features: ['G(TS)_xtb', 'DrG_xtb']\n",
      "Using file patterns: ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions:  71%|███████   | 1117/1582 [00:01<00:00, 1190.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6866 does not exist, skipping reaction_id ID79335\n",
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6867 does not exist, skipping reaction_id ID79335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions: 100%|██████████| 1582/1582 [00:01<00:00, 1127.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1580 reactions, skipped 2 reactions\n",
      "Saved metadata to dataset/DATASET_DA_F/processed/metadata.json\n",
      "Processed 1580 reactions, saved to dataset/DATASET_DA_F/processed/data_c08fa62613d2.pt\n",
      "Dataset loaded successfully with 1580 samples\n",
      "\n",
      "Sample data object from dataset:\n",
      "Data(y=[1, 2], z0=[16], z1=[16], z2=[16], pos0=[16, 3], pos1=[16, 3], pos2=[16, 3], xtb_features=[1, 2], feature_names=[2], reaction_id='ID63623', id='reaction_R0', reaction='[C:1](=[C:2]([C:3](=[C:4]([H:11])[H:12])[H:10])[H:9])([H:7])[H:8].[C:5](=[C:6]([H:15])[H:16])([H:13])[H:14]>>[C:1]1([H:7])([H:8])[C:2]([H:9])=[C:3]([H:10])[C:4]([H:11])([H:12])[C:5]([H:13])([H:14])[C:6]1([H:15])[H:16]', num_nodes=16)\n"
     ]
    }
   ],
   "source": [
    "dataset = None\n",
    "if model is not None and target_fields is not None:\n",
    "    # Check if dataset files exist\n",
    "    if not os.path.exists(dataset_root):\n",
    "        print(f\"Error: Dataset root directory not found at '{dataset_root}'\")\n",
    "    elif not os.path.exists(dataset_csv):\n",
    "        print(f\"Error: Dataset CSV file not found at '{dataset_csv}'\")\n",
    "    else:\n",
    "        print(f\"Loading dataset from {dataset_root} using CSV {dataset_csv}\")\n",
    "        try:\n",
    "            # Ensure the ReactionXYZDataset class is available from imports\n",
    "            dataset = ReactionXYZDataset(\n",
    "                root=dataset_root,\n",
    "                csv_file=dataset_csv,\n",
    "                target_fields=target_fields, # Use fields extracted from the model\n",
    "                file_patterns=file_patterns,\n",
    "                input_features=input_features, # Use features defined in config\n",
    "                id_field=id_field,\n",
    "                dir_field=dir_field,\n",
    "                reaction_field=reaction_field,\n",
    "                inference_mode=True # Crucial for prediction datasets (skips train/val/test split)\n",
    "                # Add other relevant parameters if your dataset class requires them\n",
    "            )\n",
    "            print(f\"Dataset loaded successfully with {len(dataset)} samples\")\n",
    "            \n",
    "            # Optional: Display a sample from the dataset\n",
    "            if len(dataset) > 0:\n",
    "                print(\"\\nSample data object from dataset:\")\n",
    "                print(dataset[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            print(\"Check dataset paths, CSV format, and file structure.\")\n",
    "            dataset = None # Ensure dataset is None if loading failed\n",
    "else:\n",
    "    print(\"Skipping dataset loading because the model could not be loaded or target fields are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492c857",
   "metadata": {},
   "source": [
    "## 6. Create DataLoader\n",
    "\n",
    "Prepare the data loader for batching during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "191a0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using follow_batch attributes: ['z0', 'z1', 'z2', 'pos0', 'pos1', 'pos2']\n",
      "DataLoader created with batch size 32.\n"
     ]
    }
   ],
   "source": [
    "data_loader = None\n",
    "if dataset is not None and len(dataset) > 0:\n",
    "    # Define which attributes require special batching in PyG (usually atom counts and positions)\n",
    "    # Check a sample to see which attributes exist (e.g., pos0, z0, pos1, z1, pos2, z2)\n",
    "    follow_batch = []\n",
    "    sample_data = dataset[0]\n",
    "    for key in ['z0', 'z1', 'z2', 'pos0', 'pos1', 'pos2']:\n",
    "        if hasattr(sample_data, key):\n",
    "            follow_batch.append(key)\n",
    "    print(f\"Using follow_batch attributes: {follow_batch}\")\n",
    "\n",
    "    # Create the DataLoader\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, # Important: Do not shuffle data during prediction\n",
    "        num_workers=num_workers,\n",
    "        follow_batch=follow_batch # Handles batching for varying numbers of atoms\n",
    "    )\n",
    "    print(f\"DataLoader created with batch size {batch_size}.\")\n",
    "elif dataset is not None and len(dataset) == 0:\n",
    "    print(\"Dataset is empty. Cannot create DataLoader.\")\n",
    "else:\n",
    "    print(\"Skipping DataLoader creation because the dataset was not loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c7eff",
   "metadata": {},
   "source": [
    "## 7. Run Inference\n",
    "\n",
    "Iterate through the data loader, pass batches to the model, and collect the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3da093d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "  Processed 1600 / 1580 samples...\n",
      "Inference completed. Processed 50 batches.\n",
      "Shape of concatenated scaled predictions: (1580, 2)\n"
     ]
    }
   ],
   "source": [
    "all_predictions_scaled = [] # Store predictions (likely scaled)\n",
    "all_batch_metadata = []  # Store corresponding metadata (IDs, etc.)\n",
    "\n",
    "if model is not None and data_loader is not None:\n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad(): # Disable gradient calculations for efficiency\n",
    "        batch_count = 0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device) # Move batch data to the target device\n",
    "            \n",
    "            # Extract necessary inputs for the model's forward method\n",
    "            # These names must match the attributes in your PyG Data objects\n",
    "            pos0, pos1, pos2 = batch.pos0, batch.pos1, batch.pos2\n",
    "            z0, z1, z2 = batch.z0, batch.z1, batch.z2\n",
    "            batch_mapping = batch.batch # PyG batch mapping tensor\n",
    "            \n",
    "            # Handle optional xtb_features (or other input features)\n",
    "            xtb_features = getattr(batch, 'xtb_features', None)\n",
    "            if xtb_features is not None:\n",
    "                xtb_features = xtb_features.to(device)\n",
    "            \n",
    "            # Perform the forward pass\n",
    "            # The output format depends on your Estimator's forward method\n",
    "            # Typically: embeddings, atomwise_results, predictions = model(...)\n",
    "            try:\n",
    "                 _, _, predictions = model(pos0, pos1, pos2, z0, z1, z2, batch_mapping, xtb_features)\n",
    "            except TypeError as e:\n",
    "                 print(f\"\\nError during model forward pass: {e}\")\n",
    "                 print(\"Check if the arguments passed match the model's forward signature.\")\n",
    "                 print(\"Expected arguments based on common patterns: pos0, pos1, pos2, z0, z1, z2, batch_mapping, xtb_features (optional)\")\n",
    "                 print(\"Stopping inference.\")\n",
    "                 all_predictions_scaled = [] # Clear partial results on error\n",
    "                 all_batch_metadata = []\n",
    "                 break # Exit the loop\n",
    "            except Exception as e:\n",
    "                 print(f\"\\nAn unexpected error occurred during model forward pass: {e}\")\n",
    "                 print(\"Stopping inference.\")\n",
    "                 all_predictions_scaled = []\n",
    "                 all_batch_metadata = []\n",
    "                 break\n",
    "                 \n",
    "            # Store predictions (move to CPU and convert to numpy)\n",
    "            all_predictions_scaled.append(predictions.cpu().numpy())\n",
    "            \n",
    "            # Store metadata (IDs, reaction strings, etc.) from the batch\n",
    "            batch_meta = {}\n",
    "            for attr in ['reaction_id', 'id', 'reaction']: # Add other relevant fields\n",
    "                if hasattr(batch, attr):\n",
    "                    value = getattr(batch, attr)\n",
    "                    # Ensure metadata is easily serializable (e.g., list of strings/numbers)\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        batch_meta[attr] = value.cpu().tolist() \n",
    "                    elif isinstance(value, list):\n",
    "                        batch_meta[attr] = value\n",
    "                    else: # Handle potential single values if batch size is 1\n",
    "                         batch_meta[attr] = [value] * len(predictions)\n",
    "            all_batch_metadata.append(batch_meta)\n",
    "            \n",
    "            batch_count += 1\n",
    "            if batch_count % 50 == 0: # Print progress periodically\n",
    "                 print(f\"  Processed {batch_count * batch_size} / {len(dataset)} samples...\")\n",
    "\n",
    "    if all_predictions_scaled: # Check if inference ran without breaking early\n",
    "        print(f\"Inference completed. Processed {len(all_predictions_scaled)} batches.\")\n",
    "        # Concatenate predictions from all batches\n",
    "        predictions_scaled_np = np.vstack(all_predictions_scaled)\n",
    "        print(f\"Shape of concatenated scaled predictions: {predictions_scaled_np.shape}\")\n",
    "    else:\n",
    "        print(\"Inference did not produce results (possibly due to errors or empty dataset).\")\n",
    "        predictions_scaled_np = np.array([]) # Ensure it's an empty array\n",
    "\n",
    "else:\n",
    "    print(\"Skipping inference because model or data loader is not available.\")\n",
    "    predictions_scaled_np = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60907a19",
   "metadata": {},
   "source": [
    "## 8. Process and Save Predictions\n",
    "\n",
    "Combine the predictions with identifiers, apply inverse scaling if the model has scaler information, and save the results to a CSV file and numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b92a63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing predictions...\n",
      "  Applied inverse scaling for target: 'G(TS)'\n",
      "  Applied inverse scaling for target: 'DrG'\n",
      "\n",
      "Predictions successfully saved to: ./predictions.csv\n",
      "Raw scaled predictions saved to: ./predictions/predictions_scaled.npy\n",
      "Final (unscaled) predictions saved to: ./predictions/predictions_final.npy\n",
      "\n",
      "Total number of predictions generated: 1580\n",
      "\n",
      "Sample predictions (first 5 rows):\n",
      "  reaction_id               id  \\\n",
      "0     ID63623      reaction_R0   \n",
      "1     ID86062      reaction_R1   \n",
      "2     ID52093     reaction_R10   \n",
      "3     ID31786    reaction_R100   \n",
      "4     ID30289  reaction_R10166   \n",
      "\n",
      "                                            reaction  DrG_predicted  \\\n",
      "0  [C:1](=[C:2]([C:3](=[C:4]([H:11])[H:12])[H:10]...    -140.557632   \n",
      "1  [C:6](=[C:7]([H:14])[H:15])([H:12])[H:13].[c:1...     -17.935015   \n",
      "2  [C:1]([c:2]1[c:3]([H:12])[c:4]([H:13])[c:5]([H...       7.588223   \n",
      "3  [N:6](/[C:7](=[C:8](\\[N:9]([H:20])[H:21])[H:19...     -67.345047   \n",
      "4  [C:1]([C:2](=[C:3]([C:4](=[C:5]([H:24])[H:25])...       6.474979   \n",
      "\n",
      "   G(TS)_predicted  \n",
      "0        33.314888  \n",
      "1        60.801392  \n",
      "2        64.907562  \n",
      "3        57.548111  \n",
      "4        67.294449  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "if predictions_scaled_np.size > 0 and model is not None and target_fields is not None:\n",
    "    print(\"Processing predictions...\")\n",
    "    \n",
    "    # Apply inverse scaling if scaler is available\n",
    "    predictions_final = {} # Dictionary to hold final (potentially unscaled) predictions\n",
    "    has_scaler = hasattr(model, 'scaler') and model.scaler is not None and len(model.scaler) == len(target_fields)\n",
    "\n",
    "    for i, target_name in enumerate(target_fields):\n",
    "        if i < predictions_scaled_np.shape[1]:\n",
    "            target_preds_scaled = predictions_scaled_np[:, i].reshape(-1, 1)\n",
    "            if has_scaler:\n",
    "                try:\n",
    "                    # Assume model.scaler is a list/tuple of scaler objects (e.g., StandardScaler)\n",
    "                    target_preds_unscaled = model.scaler[i].inverse_transform(target_preds_scaled)\n",
    "                    predictions_final[target_name] = target_preds_unscaled.flatten()\n",
    "                    print(f\"  Applied inverse scaling for target: '{target_name}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Could not inverse scale target '{target_name}'. Error: {e}. Using scaled values.\")\n",
    "                    predictions_final[target_name] = target_preds_scaled.flatten() # Use scaled if error\n",
    "            else:\n",
    "                predictions_final[target_name] = target_preds_scaled.flatten() # Use scaled if no scaler\n",
    "        else:\n",
    "            print(f\"  Warning: Prediction array has fewer columns ({predictions_scaled_np.shape[1]}) than target fields ({len(target_fields)}). Skipping target '{target_name}'.\")\n",
    "\n",
    "    if not has_scaler:\n",
    "         print(\"  No scaler found or scaler mismatch. Final predictions are the scaled model outputs.\")\n",
    "            \n",
    "    # Create output DataFrame\n",
    "    # Start with metadata (IDs, reaction strings, etc.)\n",
    "    df_metadata = {}\n",
    "    if all_batch_metadata:\n",
    "        # Concatenate metadata from all batches\n",
    "        first_batch_keys = all_batch_metadata[0].keys()\n",
    "        for key in first_batch_keys:\n",
    "             df_metadata[key] = [item for batch_meta in all_batch_metadata for item in batch_meta.get(key, [])]\n",
    "    \n",
    "    try:\n",
    "        results_df = pd.DataFrame(df_metadata)\n",
    "        \n",
    "        # Add predicted values (potentially unscaled)\n",
    "        for target_name, preds in predictions_final.items():\n",
    "             # Ensure length matches DataFrame length\n",
    "             if len(preds) == len(results_df):\n",
    "                  results_df[f'{target_name}_predicted'] = preds\n",
    "             else:\n",
    "                  print(f\"  Warning: Length mismatch for predicted target '{target_name}' ({len(preds)}) vs metadata ({len(results_df)}). Skipping column.\")\n",
    "                  # Optionally pad or handle mismatch differently\n",
    "                  # results_df[f'{target_name}_predicted'] = [np.nan] * len(results_df) \n",
    "\n",
    "        # Reorder columns for better readability (optional)\n",
    "        id_cols = [col for col in ['reaction_id', 'id', 'reaction'] if col in results_df.columns]\n",
    "        pred_cols = sorted([col for col in results_df.columns if col.endswith('_predicted')])\n",
    "        other_cols = sorted([col for col in results_df.columns if col not in id_cols and col not in pred_cols])\n",
    "        results_df = results_df[id_cols + pred_cols + other_cols]\n",
    "        \n",
    "        # Save predictions\n",
    "        results_df.to_csv(output_csv, index=False)\n",
    "        print(f\"\\nPredictions successfully saved to: {output_csv}\")\n",
    "        \n",
    "        # Save raw scaled predictions as numpy array\n",
    "        scaled_npy_path = os.path.join(output_dir, 'predictions_scaled.npy')\n",
    "        np.save(scaled_npy_path, predictions_scaled_np)\n",
    "        print(f\"Raw scaled predictions saved to: {scaled_npy_path}\")\n",
    "\n",
    "        # Save final (potentially unscaled) predictions as numpy array\n",
    "        if predictions_final:\n",
    "            try:\n",
    "                 # Ensure all arrays have the same length before stacking\n",
    "                 valid_preds = {k: v for k, v in predictions_final.items() if len(v) == predictions_scaled_np.shape[0]}\n",
    "                 if len(valid_preds) == len(target_fields):\n",
    "                    final_preds_array = np.stack([valid_preds[key] for key in target_fields], axis=1)\n",
    "                    final_npy_path = os.path.join(output_dir, 'predictions_final.npy')\n",
    "                    np.save(final_npy_path, final_preds_array)\n",
    "                    print(f\"Final (unscaled) predictions saved to: {final_npy_path}\")\n",
    "                 else:\n",
    "                    print(\"Could not save final predictions numpy array due to inconsistent lengths or missing targets.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving final predictions numpy array: {e}\")\n",
    "\n",
    "        print(f\"\\nTotal number of predictions generated: {len(results_df)}\")\n",
    "        if len(results_df) > 0:\n",
    "            print(\"\\nSample predictions (first 5 rows):\")\n",
    "            print(results_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or saving DataFrame: {e}\")\n",
    "        print(\"Please check the collected metadata and predictions.\")\n",
    "\n",
    "elif predictions_scaled_np.size == 0:\n",
    "    print(\"No predictions were generated, skipping saving.\")\n",
    "else:\n",
    "    print(\"Model or target fields missing, skipping processing and saving.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dde24-850b-4be2-9b8b-e272965f4f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aeb909-758a-46a2-9c87-a1d1ddf2c153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
