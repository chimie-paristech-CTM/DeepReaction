{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24865530",
   "metadata": {},
   "source": [
    "# Prediction Demo for DeepReaction\n",
    "\n",
    "This notebook demonstrates how to use a trained molecular reaction prediction model to make predictions using the DeepReaction framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da61b8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6d7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from deepreaction package\n",
    "from deepreaction.config.config import Config\n",
    "from deepreaction.data.load_Reaction import load_reaction_for_inference\n",
    "from torch_geometric.loader import DataLoader\n",
    "from deepreaction.core.predictor import ReactionPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9df28",
   "metadata": {},
   "source": [
    "## 2. Define Prediction Parameters\n",
    "\n",
    "All parameters are defined in a single dictionary for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all parameters in a single dictionary\n",
    "params = {\n",
    "    # Dataset parameters\n",
    "    'dataset_root': './dataset/DATASET_DA_F',  # Adjust path if needed\n",
    "    'dataset_csv': './dataset/DATASET_DA_F/dataset_xtb_final.csv', # Adjust path if needed\n",
    "    'target_fields': ['G(TS)', 'DrG'],\n",
    "    'input_features': ['G(TS)_xtb', 'DrG_xtb'],\n",
    "    'file_patterns': ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz'],\n",
    "    'id_field': 'ID',\n",
    "    'dir_field': 'R_dir',\n",
    "    'reaction_field': 'reaction',\n",
    "    'random_seed': 42234,\n",
    "    'inference_mode': True,  # Important: set to True for prediction\n",
    "    \n",
    "    # Prediction parameters\n",
    "    'checkpoint_path': './results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1359-v1.ckpt',  # Path to trained model\n",
    "    'output_dir': './predictions',  # Output directory for prediction results\n",
    "    'batch_size': 16,\n",
    "    'use_scaler': True,  # Should match what was used during training\n",
    "    'num_workers': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867a4d2",
   "metadata": {},
   "source": [
    "## 3. Set Up GPU and Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b699333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090 D\n",
      "Output directory created/exists: ./predictions\n"
     ]
    }
   ],
   "source": [
    "# Setup GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    params['gpu'] = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    params['gpu'] = False\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(params['output_dir'], exist_ok=True)\n",
    "print(f\"Output directory created/exists: {params['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099063b8",
   "metadata": {},
   "source": [
    "## 4. Load Data Directly for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff8ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created successfully\n",
      "Loading data directly for inference...\n",
      "Error checking saved data: 'NoneType' object is not subscriptable\n",
      "Inference mode: Using dummy target field\n",
      "Using target fields: ['target']\n",
      "Using input features: ['G(TS)_xtb', 'DrG_xtb']\n",
      "Using file patterns: ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions:  71%|███████   | 1126/1582 [00:00<00:00, 1873.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6866 does not exist, skipping reaction_id ID79335\n",
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6867 does not exist, skipping reaction_id ID79335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions: 100%|██████████| 1582/1582 [00:00<00:00, 1851.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1580 reactions, skipped 2 reactions\n",
      "Saved metadata to dataset/DATASET_DA_F/processed/metadata.json\n",
      "Processed 1580 reactions, saved to dataset/DATASET_DA_F/processed/data_d9139bd83f9f.pt\n",
      "Loaded 1580 samples for inference\n",
      "Created data loader with 99 batches\n"
     ]
    }
   ],
   "source": [
    "# Creating configuration is still useful for record-keeping\n",
    "config = Config.from_params(params)\n",
    "print(\"Configuration created successfully\")\n",
    "\n",
    "# Load data directly using load_reaction_for_inference instead of ReactionDataset\n",
    "print(\"Loading data directly for inference...\")\n",
    "inference_data = load_reaction_for_inference(\n",
    "    random_seed=params['random_seed'],\n",
    "    root=params['dataset_root'],\n",
    "    dataset_csv=params['dataset_csv'],\n",
    "    file_patterns=params['file_patterns'],\n",
    "    input_features=params['input_features'],\n",
    "    id_field=params['id_field'],\n",
    "    dir_field=params['dir_field'],\n",
    "    reaction_field=params['reaction_field']\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(inference_data)} samples for inference\")\n",
    "\n",
    "# Create a data loader for prediction\n",
    "follow_batch = ['z0', 'z1', 'z2', 'pos0', 'pos1', 'pos2']\n",
    "inference_loader = DataLoader(\n",
    "    inference_data,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=params['num_workers'],\n",
    "    follow_batch=follow_batch\n",
    ")\n",
    "\n",
    "print(f\"Created data loader with {len(inference_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f587e70",
   "metadata": {},
   "source": [
    "## 5. Verify Checkpoint Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fee877",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Checkpoint not found: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1359-v1.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check that the model checkpoint file exists\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_path\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Checkpoint not found: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1359-v1.ckpt"
     ]
    }
   ],
   "source": [
    "# Check that the model checkpoint file exists\n",
    "if not os.path.exists(params['checkpoint_path']):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {params['checkpoint_path']}\")\n",
    "else:\n",
    "    print(f\"Found checkpoint: {params['checkpoint_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37733f",
   "metadata": {},
   "source": [
    "## 6. Initialize Predictor and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor with the trained model\n",
    "predictor = ReactionPredictor(\n",
    "    checkpoint_path=params['checkpoint_path'],\n",
    "    output_dir=params['output_dir'],\n",
    "    batch_size=params['batch_size'],\n",
    "    gpu=params['gpu'],\n",
    "    num_workers=params['num_workers'],\n",
    "    use_scaler=params['use_scaler']\n",
    ")\n",
    "print(\"Predictor initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a25f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction directly on the data loader\n",
    "print(\"Starting prediction...\")\n",
    "predictions_df = predictor.predict(\n",
    "    data_loader=inference_loader,\n",
    "    csv_output_path=os.path.join(params['output_dir'], 'predictions.csv')\n",
    ")\n",
    "print(\"Prediction completed successfully\")\n",
    "print(f\"Predictions saved to {os.path.join(params['output_dir'], 'predictions.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98cfd3",
   "metadata": {},
   "source": [
    "## 7. Analyze Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prediction results\n",
    "print(\"\\nPreview of predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Calculate and display basic statistics for each target\n",
    "target_stats = {}\n",
    "for target in params['target_fields']:\n",
    "    pred_col = f\"{target}_predicted\"\n",
    "    if pred_col in predictions_df.columns:\n",
    "        pred_values = predictions_df[pred_col].values\n",
    "        stats = {\n",
    "            'min': pred_values.min(),\n",
    "            'max': pred_values.max(),\n",
    "            'mean': pred_values.mean(),\n",
    "            'std': pred_values.std()\n",
    "        }\n",
    "        target_stats[target] = stats\n",
    "        \n",
    "        print(f\"\\nStatistics for {pred_col}:\")\n",
    "        print(f\"Min: {stats['min']:.4f}\")\n",
    "        print(f\"Max: {stats['max']:.4f}\")\n",
    "        print(f\"Mean: {stats['mean']:.4f}\")\n",
    "        print(f\"Std: {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c95423",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of prediction distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, target in enumerate(params['target_fields']):\n",
    "    pred_col = f\"{target}_predicted\"\n",
    "    if pred_col in predictions_df.columns:\n",
    "        plt.subplot(1, len(params['target_fields']), i+1)\n",
    "        plt.hist(predictions_df[pred_col], bins=20, alpha=0.7)\n",
    "        plt.title(f\"{target} Predictions\")\n",
    "        plt.xlabel(\"Predicted Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.axvline(x=target_stats[target]['mean'], color='r', linestyle='--', \n",
    "                   label=f\"Mean: {target_stats[target]['mean']:.2f}\")\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(params['output_dir'], 'prediction_histogram.png'))\n",
    "print(f\"Saved prediction histogram to {os.path.join(params['output_dir'], 'prediction_histogram.png')}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation plots between input features and predictions\n",
    "if params['input_features']:\n",
    "    try:\n",
    "        # Load original CSV to get input features\n",
    "        original_df = pd.read_csv(params['dataset_csv'])\n",
    "        \n",
    "        # Merge with predictions\n",
    "        merged_df = pd.merge(\n",
    "            predictions_df, \n",
    "            original_df[[params['id_field']] + params['input_features']], \n",
    "            on=params['id_field'], \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        if not merged_df.empty:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plot_idx = 1\n",
    "            \n",
    "            for target in params['target_fields']:\n",
    "                pred_col = f\"{target}_predicted\"\n",
    "                \n",
    "                for feature in params['input_features']:\n",
    "                    if pred_col in merged_df.columns and feature in merged_df.columns:\n",
    "                        plt.subplot(len(params['target_fields']), len(params['input_features']), plot_idx)\n",
    "                        plt.scatter(merged_df[feature], merged_df[pred_col], alpha=0.5)\n",
    "                        plt.title(f\"{target} vs {feature}\")\n",
    "                        plt.xlabel(feature)\n",
    "                        plt.ylabel(f\"{target} (Predicted)\")\n",
    "                        \n",
    "                        # Add correlation line\n",
    "                        z = np.polyfit(merged_df[feature], merged_df[pred_col], 1)\n",
    "                        p = np.poly1d(z)\n",
    "                        plt.plot(merged_df[feature], p(merged_df[feature]), \"r--\")\n",
    "                        \n",
    "                        corr = np.corrcoef(merged_df[feature], merged_df[pred_col])[0, 1]\n",
    "                        plt.annotate(f\"Corr: {corr:.2f}\", xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "                        \n",
    "                        plot_idx += 1\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(params['output_dir'], 'feature_correlation.png'))\n",
    "            print(f\"Saved feature correlation plot to {os.path.join(params['output_dir'], 'feature_correlation.png')}\")\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating correlation plots: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa523b6d",
   "metadata": {},
   "source": [
    "## 9. Compare with Original Values (If Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare predictions with original values if available\n",
    "try:\n",
    "    # Load original CSV\n",
    "    original_df = pd.read_csv(params['dataset_csv'])\n",
    "    \n",
    "    # Merge with predictions\n",
    "    comparison_df = pd.merge(\n",
    "        predictions_df,\n",
    "        original_df[[params['id_field']] + params['target_fields'] + params['input_features']],\n",
    "        on=params['id_field'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    if not comparison_df.empty:\n",
    "        # Create scatter plots for predicted vs actual values\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        for i, target in enumerate(params['target_fields']):\n",
    "            pred_col = f\"{target}_predicted\"\n",
    "            \n",
    "            if pred_col in comparison_df.columns and target in comparison_df.columns:\n",
    "                plt.subplot(1, len(params['target_fields']), i+1)\n",
    "                \n",
    "                # Get min and max for setting plot limits\n",
    "                min_val = min(comparison_df[target].min(), comparison_df[pred_col].min())\n",
    "                max_val = max(comparison_df[target].max(), comparison_df[pred_col].max())\n",
    "                \n",
    "                # Plot points\n",
    "                plt.scatter(comparison_df[target], comparison_df[pred_col], alpha=0.5)\n",
    "                \n",
    "                # Plot diagonal line (perfect prediction)\n",
    "                plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                \n",
    "                # Calculate metrics\n",
    "                from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "                mae = mean_absolute_error(comparison_df[target], comparison_df[pred_col])\n",
    "                rmse = mean_squared_error(comparison_df[target], comparison_df[pred_col], squared=False)\n",
    "                r2 = r2_score(comparison_df[target], comparison_df[pred_col])\n",
    "                \n",
    "                plt.title(f\"{target} (Actual vs Predicted)\\nMAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "                plt.xlabel(f\"Actual {target}\")\n",
    "                plt.ylabel(f\"Predicted {target}\")\n",
    "                \n",
    "                # Add grid for better readability\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(params['output_dir'], 'actual_vs_predicted.png'))\n",
    "        print(f\"Saved actual vs predicted plot to {os.path.join(params['output_dir'], 'actual_vs_predicted.png')}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Save comparison to CSV\n",
    "        comparison_csv = os.path.join(params['output_dir'], 'comparison.csv')\n",
    "        comparison_df.to_csv(comparison_csv, index=False)\n",
    "        print(f\"Saved comparison data to {comparison_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating comparison plots: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaction",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
