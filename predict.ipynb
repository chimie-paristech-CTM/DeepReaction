{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reaction Model Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.insert(0, str(Path('.').absolute()))\n",
    "\n",
    "from deepreaction import Config, ReactionPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Set up the configuration for inference. These parameters should match those used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters loaded successfully!\n",
      "Features to use: ['DG_act_xtb', 'DrG_xtb']\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Inference configuration parameters\n",
    "params = {\n",
    "    # Dataset configuration\n",
    "    'dataset': 'XTB',\n",
    "    'readout': 'mean',\n",
    "    'dataset_root': './dataset/DATASET_DA_F',  \n",
    "    'input_features': ['DG_act_xtb', 'DrG_xtb'],  # XTB-computed features\n",
    "    'file_patterns': ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz'],\n",
    "    'file_dir_pattern': 'reaction_*',\n",
    "    'id_field': 'ID',\n",
    "    'dir_field': 'R_dir',\n",
    "    'reaction_field': 'smiles',  # SMILES representation field\n",
    "    'use_scaler': True,  # Use the same scaler as training\n",
    "    \n",
    "    # Inference settings\n",
    "    'batch_size': 32,  # Can be larger for inference\n",
    "    'random_seed': 42234,\n",
    "    \n",
    "    # Hardware configuration\n",
    "    'cuda': True,\n",
    "    'gpu_id': 0,\n",
    "    'num_workers': 4,\n",
    "    'log_level': 'info'\n",
    "}\n",
    "\n",
    "print(\"Configuration parameters loaded successfully!\")\n",
    "print(f\"Features to use: {params['input_features']}\")\n",
    "print(f\"Batch size: {params['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. File Paths Configuration\n",
    "\n",
    "Specify the paths for the trained model, input data, and output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1134.ckpt\n",
      "Input data: ./dataset/DATASET_DA_F/dataset_xtb_final.csv\n",
      "Output directory: ./predictions\n",
      "Output directory created/verified: ./predictions\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "checkpoint_path = \"./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1134.ckpt\"\n",
    "inference_csv = \"./dataset/DATASET_DA_F/dataset_xtb_final.csv\"     \n",
    "output_dir = \"./predictions\"\n",
    "\n",
    "print(f\"Model checkpoint: {checkpoint_path}\")\n",
    "print(f\"Input data: {inference_csv}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory created/verified: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Device Setup\n",
    "\n",
    "Configure GPU/CPU usage for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "GPU Memory: 12.6 GB\n",
      "PyTorch version: 2.3.0+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Setup device (GPU/CPU)\n",
    "if params['cuda'] and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(params['gpu_id'])\n",
    "    device = torch.device(f\"cuda:{params['gpu_id']}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    params['cuda'] = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Input Files\n",
    "\n",
    "Check that all required files exist before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model checkpoint: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1134.ckpt (39.9 MB)\n",
      "✓ Inference CSV: ./dataset/DATASET_DA_F/dataset_xtb_final.csv (0.8 MB)\n",
      "\n",
      "✓ All required files found!\n"
     ]
    }
   ],
   "source": [
    "# Verify that required files exist\n",
    "files_to_check = {\n",
    "    \"Model checkpoint\": checkpoint_path,\n",
    "    \"Inference CSV\": inference_csv\n",
    "}\n",
    "\n",
    "all_files_exist = True\n",
    "for file_type, file_path in files_to_check.items():\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
    "        print(f\"✓ {file_type}: {file_path} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"✗ {file_type}: {file_path} - FILE NOT FOUND\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    raise FileNotFoundError(\"One or more required files are missing. Please check the paths above.\")\n",
    "    \n",
    "print(\"\\n✓ All required files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Configuration and Predictor\n",
    "\n",
    "Create the configuration object and load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 13:49:19,861 - deepreaction - INFO - Loading model from checkpoint: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1134.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating configuration...\n",
      "Configuration created successfully!\n",
      "\n",
      "Loading model from: ./results/reaction_model/checkpoints/best-epoch=0002-val_total_loss=0.1134.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 13:49:20,466 - deepreaction - INFO - Model target fields: ['DG_act', 'DrG']\n",
      "2025-05-23 13:49:20,467 - deepreaction - INFO - Model has 2 scalers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Model Configuration:\n",
      "- Dataset type: XTB\n",
      "- Input features: ['DG_act_xtb', 'DrG_xtb']\n",
      "- Batch size: 32\n",
      "- Using scaler: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating configuration...\")\n",
    "config = Config.from_params(params)\n",
    "print(\"Configuration created successfully!\")\n",
    "\n",
    "print(f\"\\nLoading model from: {checkpoint_path}\")\n",
    "predictor = ReactionPredictor(config=config, checkpoint_path=checkpoint_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"- Dataset type: {config.dataset.dataset}\")\n",
    "print(f\"- Input features: {config.dataset.input_features}\")\n",
    "print(f\"- Batch size: {config.training.batch_size}\")\n",
    "print(f\"- Using scaler: {config.dataset.use_scaler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Predictions\n",
    "\n",
    "Execute the prediction process on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 13:49:20,473 - deepreaction - INFO - Loading inference data from: ./dataset/DATASET_DA_F/dataset_xtb_final.csv\n",
      "2025-05-23 13:49:20,475 - deepreaction - INFO - Loading inference dataset from ./dataset/DATASET_DA_F/dataset_xtb_final.csv\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n",
      "Processing data from: ./dataset/DATASET_DA_F/dataset_xtb_final.csv\n",
      "Output will be saved to: ./predictions\n",
      "Inference mode: Using dummy target field\n",
      "Using target fields: ['target']\n",
      "Using input features: ['DG_act_xtb', 'DrG_xtb']\n",
      "Using file suffixes: reactant='_reactant.xyz', ts='_ts.xyz', product='_product.xyz'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions:  80%|███████▉  | 1258/1582 [00:00<00:00, 2028.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6866 does not exist, skipping reaction_id ID79335\n",
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6867 does not exist, skipping reaction_id ID79335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions: 100%|██████████| 1582/1582 [00:00<00:00, 2049.52it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata to dataset/DATASET_DA_F/processed/metadata.json\n",
      "Processed 1580 reactions, saved to dataset/DATASET_DA_F/processed/data_7577d801.pt\n",
      "Null data at index 0\n",
      "Removing old processed file: dataset/DATASET_DA_F/processed/data_7577d801.pt\n",
      "Inference mode: Using dummy target field\n",
      "Using target fields: ['target']\n",
      "Using input features: ['DG_act_xtb', 'DrG_xtb']\n",
      "Using file suffixes: reactant='_reactant.xyz', ts='_ts.xyz', product='_product.xyz'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions:  78%|███████▊  | 1240/1582 [00:00<00:00, 1924.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6866 does not exist, skipping reaction_id ID79335\n",
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6867 does not exist, skipping reaction_id ID79335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions: 100%|██████████| 1582/1582 [00:00<00:00, 1592.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata to dataset/DATASET_DA_F/processed/metadata.json\n",
      "Processed 1580 reactions, saved to dataset/DATASET_DA_F/processed/data_7577d801.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 13:49:23,278 - deepreaction - INFO - Loaded 1580 samples for inference\n",
      "2025-05-23 13:49:23,282 - deepreaction - INFO - Running predictions on 1580 samples...\n",
      "2025-05-23 13:49:29,754 - deepreaction - INFO - Predictions shape: (1580, 2)\n",
      "2025-05-23 13:49:29,755 - deepreaction - INFO - Mapping input features ['DG_act_xtb', 'DrG_xtb'] to model targets ['DG_act', 'DrG']\n",
      "2025-05-23 13:49:29,756 - deepreaction - INFO - Mapped feature 'DG_act_xtb' (key: 'DG_act') to scaler index 0\n",
      "2025-05-23 13:49:29,757 - deepreaction - INFO - Mapped feature 'DrG_xtb' (key: 'DrG') to scaler index 1\n",
      "2025-05-23 13:49:29,758 - deepreaction - INFO - Output field names: ['DG_act', 'DrG']\n",
      "2025-05-23 13:49:29,760 - deepreaction - INFO - Applied inverse scaling (scaler 0) for output field 'DG_act'\n",
      "2025-05-23 13:49:29,762 - deepreaction - INFO - Applied inverse scaling (scaler 1) for output field 'DrG'\n",
      "2025-05-23 13:49:29,826 - deepreaction - INFO - Predictions saved to ./predictions/predictions.csv\n",
      "2025-05-23 13:49:29,827 - deepreaction - INFO - Result statistics:\n",
      "2025-05-23 13:49:29,829 - deepreaction - INFO -   DG_act: mean=37.0567, std=7.9769\n",
      "2025-05-23 13:49:29,830 - deepreaction - INFO -   DrG: mean=-5.2128, std=16.1594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Prediction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting prediction...\")\n",
    "print(f\"Processing data from: {inference_csv}\")\n",
    "print(f\"Output will be saved to: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    # Run predictions\n",
    "    results = predictor.predict_from_csv(inference_csv, output_dir=output_dir)\n",
    "    \n",
    "    prediction_success = True\n",
    "    print(\"\\n✓ Prediction completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Prediction failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    prediction_success = False\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results\n",
    "\n",
    "Display and analyze the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "Results shape: (1580, 5)\n",
      "Number of predictions: 1580\n",
      "Number of features: 5\n",
      "\n",
      "Columns in results:\n",
      "  1. ID\n",
      "  2. id\n",
      "  3. smiles\n",
      "  4. DG_act_predicted\n",
      "  5. DrG_predicted\n",
      "\n",
      "Results saved to: ./predictions/predictions.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if prediction_success and results is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Results shape: {results.shape}\")\n",
    "    print(f\"Number of predictions: {len(results)}\")\n",
    "    print(f\"Number of features: {results.shape[1]}\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nColumns in results:\")\n",
    "    for i, col in enumerate(results.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_dir}/predictions.csv\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREDICTION FAILED\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Please check the error messages above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Sample Results\n",
    "\n",
    "Show the first few prediction results for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
