{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24865530",
   "metadata": {},
   "source": [
    "# Training Demo for DeepReaction\n",
    "\n",
    "This notebook demonstrates how to train a molecular reaction prediction model using the DeepReaction framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da61b8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6d7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming deepreaction is installed or in the Python path\n",
    "# If not, you might need to add its location to sys.path\n",
    "# Example: sys.path.append('/path/to/deepreaction/parent/directory')\n",
    "from deepreaction import ReactionDataset, ReactionTrainer\n",
    "from deepreaction.config import ReactionConfig, ModelConfig, TrainingConfig, Config, save_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9df28",
   "metadata": {},
   "source": [
    "## 2. Define Training Parameters\n",
    "\n",
    "The parameters below can be modified to fit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration parameters - these can be modified directly in the notebook\n",
    "params = {\n",
    "    # Dataset parameters\n",
    "    'dataset': 'XTB',\n",
    "    'readout': 'mean',\n",
    "    'dataset_root': './dataset/DATASET_DA_F',  # Adjust path if needed\n",
    "    'dataset_csv': './dataset/DATASET_DA_F/dataset_xtb_final.csv', # Adjust path if needed\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1,\n",
    "    'test_ratio': 0.1,\n",
    "    'target_fields': ['G(TS)', 'DrG'],\n",
    "    'target_weights': [1.0, 1.0],\n",
    "    'input_features': ['G(TS)_xtb', 'DrG_xtb'],\n",
    "    'file_patterns': ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz'],\n",
    "    'file_dir_pattern': 'reaction_*',\n",
    "    'id_field': 'ID',\n",
    "    'dir_field': 'R_dir',\n",
    "    'reaction_field': 'reaction',\n",
    "    'cv_folds': 0, # Set > 0 for cross-validation\n",
    "    \n",
    "    # Model parameters (DimeNet++ specific)\n",
    "    'model_type': 'dimenet++',\n",
    "    'node_dim': 128,\n",
    "    'dropout': 0.1,\n",
    "    'prediction_hidden_layers': 3,\n",
    "    'prediction_hidden_dim': 512,\n",
    "    'use_layer_norm': False,\n",
    "    \n",
    "    'hidden_channels': 128,\n",
    "    'num_blocks': 5,\n",
    "    'int_emb_size': 64,\n",
    "    'basis_emb_size': 8,\n",
    "    'out_emb_channels': 256,\n",
    "    'num_spherical': 7,\n",
    "    'num_radial': 6,\n",
    "    'cutoff': 5.0,\n",
    "    'envelope_exponent': 5,\n",
    "    'num_before_skip': 1,\n",
    "    'num_after_skip': 2,\n",
    "    'num_output_layers': 3,\n",
    "    'max_num_neighbors': 32,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': None, # Uses batch_size if None\n",
    "    'lr': 0.0005,\n",
    "    'finetune_lr': None,\n",
    "    'epochs': 1, # Set higher for actual training (e.g., 100, 500)\n",
    "    'min_epochs': 0,\n",
    "    'early_stopping': 40,\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'warmup_cosine',\n",
    "    'warmup_epochs': 10,\n",
    "    'min_lr': 1e-7,\n",
    "    'weight_decay': 0.0001,\n",
    "    'random_seed': 42234,\n",
    "    \n",
    "    'out_dir': './results/reaction_model', # Adjust path if needed\n",
    "    'save_best_model': True,\n",
    "    'save_last_model': False,\n",
    "    'checkpoint_path': None, # Path to a .ckpt file to resume/continue\n",
    "    'mode': 'continue', # 'train' or 'continue'\n",
    "    'freeze_base_model': False,\n",
    "    \n",
    "    'cuda': True, # Set to False to force CPU\n",
    "    'gpu_id': 0,\n",
    "    'num_workers': 4 # Number of workers for data loading\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867a4d2",
   "metadata": {},
   "source": [
    "## 3. Set Up GPU and Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b699333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA TITAN Xp\n",
      "Output directory created/exists: ./results/reaction_model\n"
     ]
    }
   ],
   "source": [
    "# Setup GPU or CPU\n",
    "if params['cuda'] and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(params['gpu_id'])\n",
    "    device = torch.device(f\"cuda:{params['gpu_id']}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    params['cuda'] = False # Ensure cuda param reflects reality\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(params['out_dir'], exist_ok=True)\n",
    "print(f\"Output directory created/exists: {params['out_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099063b8",
   "metadata": {},
   "source": [
    "## 4. Create Configuration Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff8ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to ./results/reaction_model/config.yaml and ./results/reaction_model/config.json\n"
     ]
    }
   ],
   "source": [
    "# Convert parameters to configuration objects\n",
    "reaction_config = ReactionConfig(\n",
    "    dataset_root=params['dataset_root'],\n",
    "    dataset_csv=params['dataset_csv'],\n",
    "    target_fields=params['target_fields'],\n",
    "    file_patterns=params['file_patterns'],\n",
    "    input_features=params['input_features'],\n",
    "    use_scaler=True,\n",
    "    train_ratio=params['train_ratio'],\n",
    "    val_ratio=params['val_ratio'],\n",
    "    test_ratio=params['test_ratio'],\n",
    "    cv_folds=params['cv_folds'],\n",
    "    cv_test_fold=-1, # Which fold is test set in CV, -1 if standard split\n",
    "    cv_stratify=False,\n",
    "    cv_grouped=True,\n",
    "    id_field=params['id_field'],\n",
    "    dir_field=params['dir_field'],\n",
    "    reaction_field=params['reaction_field'],\n",
    "    random_seed=params['random_seed']\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_type=params['model_type'],\n",
    "    readout=params['readout'],\n",
    "    # DimeNet++ specific\n",
    "    hidden_channels=params['hidden_channels'],\n",
    "    num_blocks=params['num_blocks'],\n",
    "    cutoff=params['cutoff'],\n",
    "    int_emb_size=params['int_emb_size'],\n",
    "    basis_emb_size=params['basis_emb_size'],\n",
    "    out_emb_channels=params['out_emb_channels'],\n",
    "    num_spherical=params['num_spherical'],\n",
    "    num_radial=params['num_radial'],\n",
    "    envelope_exponent=params['envelope_exponent'],\n",
    "    num_before_skip=params['num_before_skip'],\n",
    "    num_after_skip=params['num_after_skip'],\n",
    "    num_output_layers=params['num_output_layers'],\n",
    "    max_num_neighbors=params['max_num_neighbors'],\n",
    "    # General model params\n",
    "    node_dim=params['node_dim'], \n",
    "    dropout=params['dropout'],\n",
    "    use_layer_norm=params['use_layer_norm'],\n",
    "    use_xtb_features=len(params['input_features']) > 0,\n",
    "    num_xtb_features=len(params['input_features']),\n",
    "    prediction_hidden_layers=params['prediction_hidden_layers'],\n",
    "    prediction_hidden_dim=params['prediction_hidden_dim']\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    output_dir=params['out_dir'],\n",
    "    batch_size=params['batch_size'],\n",
    "    learning_rate=params['lr'],\n",
    "    max_epochs=params['epochs'],\n",
    "    min_epochs=params['min_epochs'],\n",
    "    early_stopping_patience=params['early_stopping'],\n",
    "    save_best_model=params['save_best_model'],\n",
    "    save_last_model=params['save_last_model'],\n",
    "    optimizer=params['optimizer'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "    scheduler=params['scheduler'],\n",
    "    warmup_epochs=params['warmup_epochs'],\n",
    "    min_lr=params['min_lr'],\n",
    "    target_weights=params['target_weights'],\n",
    "    gpu=params['cuda'],\n",
    "    num_workers=params['num_workers'],\n",
    "    resume_from_checkpoint=params['checkpoint_path']\n",
    ")\n",
    "\n",
    "config = Config(\n",
    "    reaction=reaction_config,\n",
    "    model=model_config,\n",
    "    training=training_config\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(params['out_dir'], 'config')\n",
    "save_config(config, config_path) # Saves both .yaml and .json\n",
    "print(f\"Configuration saved to {config_path}.yaml and {config_path}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f587e70",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Dataset\n",
    "\n",
    "*Note: This might take a while depending on the dataset size and preprocessing steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fee877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./dataset/DATASET_DA_F\n",
      "Error checking saved data: 'NoneType' object is not subscriptable\n",
      "Using target fields: ['G(TS)', 'DrG']\n",
      "Using input features: ['G(TS)_xtb', 'DrG_xtb']\n",
      "Using file patterns: ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions:  63%|██████▎   | 1002/1582 [00:00<00:00, 1155.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6866 does not exist, skipping reaction_id ID79335\n",
      "Warning: Folder dataset/DATASET_DA_F/reaction_R6867 does not exist, skipping reaction_id ID79335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reactions: 100%|██████████| 1582/1582 [00:01<00:00, 1112.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1580 reactions, skipped 2 reactions\n",
      "Saved metadata to dataset/DATASET_DA_F/processed/metadata.json\n",
      "Processed 1580 reactions, saved to dataset/DATASET_DA_F/processed/data_038b0f2fed6b.pt\n",
      "Dataset split: train 1269, validation 162, test 149 samples\n",
      "Dataset loaded successfully\n",
      "Dataset stats: Train: 1269, Validation: 162, Test: 149\n",
      "\n",
      "Sample data attributes:\n",
      "  reaction_id: ID87464\n",
      "  id: reaction_R13963\n",
      "  reaction: [C:1]([C:2](=[C:3]([C:4]([H:23])([H:24])[H:25])[C:5](=[C:6]([O:7][H:28])[H:27])[H:26])[H:22])([H:19])([H:20])[H:21].[O:8]=[C:9]1[N:10]([C:11]([O:12][H:31])([H:29])[H:30])[C:17](=[O:18])[C:15]([C:16]([H:32])([H:33])[H:34])=[C:13]1[C:14]([H:35])([H:36])[H:37]>>[C:1]([C@:2]1([H:22])[C:3]([C:4]([H:23])([H:24])[H:25])=[C:5]([H:26])[C@@:6]([O:7][H:28])([H:27])[C@@:13]2([C:14]([H:35])([H:36])[H:37])[C:9](=[O:8])[N:10]([C:11]([O:12][H:31])([H:29])[H:30])[C:17](=[O:18])[C@@:15]12[C:16]([H:32])([H:33])[H:34])([H:19])([H:20])[H:21]\n",
      "  y: shape: torch.Size([1, 2])\n",
      "  pos0: shape: torch.Size([37, 3])\n",
      "  z0: shape: torch.Size([37])\n",
      "  pos1: shape: torch.Size([37, 3])\n",
      "  z1: shape: torch.Size([37])\n",
      "  pos2: shape: torch.Size([37, 3])\n",
      "  z2: shape: torch.Size([37])\n",
      "  xtb_features: shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset from {params['dataset_root']}\")\n",
    "# Ensure file paths exist before proceeding\n",
    "if not os.path.exists(params['dataset_root']) or not os.path.exists(params['dataset_csv']):\n",
    "    print(f\"Error: Dataset root ({params['dataset_root']}) or CSV ({params['dataset_csv']}) not found.\")\n",
    "    print(\"Please ensure the dataset files are correctly placed and paths are updated in Section 2.\")\n",
    "    # Stop execution or raise an error if critical files are missing\n",
    "    # raise FileNotFoundError(\"Dataset files not found.\") \n",
    "else:\n",
    "    dataset = ReactionDataset(\n",
    "        root=params['dataset_root'],\n",
    "        csv_file=params['dataset_csv'],\n",
    "        target_fields=params['target_fields'],\n",
    "        file_patterns=params['file_patterns'],\n",
    "        input_features=params['input_features'],\n",
    "        use_scaler=True, # Important for consistent scaling\n",
    "        random_seed=params['random_seed'],\n",
    "        train_ratio=params['train_ratio'],\n",
    "        val_ratio=params['val_ratio'],\n",
    "        test_ratio=params['test_ratio'],\n",
    "        cv_folds=params['cv_folds'],\n",
    "        id_field=params['id_field'],\n",
    "        dir_field=params['dir_field'],\n",
    "        reaction_field=params['reaction_field']\n",
    "        # Add other ReactionDataset specific args if needed\n",
    "    )\n",
    "\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    data_stats = dataset.get_data_stats()\n",
    "    print(f\"Dataset stats: Train: {data_stats['train_size']}, Validation: {data_stats['val_size']}, Test: {data_stats['test_size']}\")\n",
    "    if params['cv_folds'] > 0:\n",
    "        print(f\"Cross-validation enabled with {dataset.get_num_folds()} folds.\")\n",
    "\n",
    "    # Show sample data (optional)\n",
    "    try:\n",
    "        if dataset.train_data and len(dataset.train_data) > 0:\n",
    "            sample = dataset.train_data[0]\n",
    "            print(f\"\\nSample data attributes:\")\n",
    "            # Print common attributes expected in a PyG Data object for reactions\n",
    "            for attr in ['reaction_id', 'id', 'reaction', 'y', 'pos0', 'z0', 'pos1', 'z1', 'pos2', 'z2', 'xtb_features']:\n",
    "                if hasattr(sample, attr):\n",
    "                     val = getattr(sample, attr)\n",
    "                     val_repr = f\"shape: {val.shape}\" if isinstance(val, torch.Tensor) else val\n",
    "                     print(f\"  {attr}: {val_repr}\")\n",
    "        elif dataset.data and len(dataset.data) > 0 and params['cv_folds'] > 0:\n",
    "             # If using CV, train_data might be None initially, check raw data\n",
    "             sample = dataset.data[0]\n",
    "             print(f\"\\nSample data attributes (from raw dataset for CV):\")\n",
    "             for attr in ['reaction_id', 'id', 'reaction', 'y', 'pos0', 'z0', 'pos1', 'z1', 'pos2', 'z2', 'xtb_features']:\n",
    "                 if hasattr(sample, attr):\n",
    "                     val = getattr(sample, attr)\n",
    "                     val_repr = f\"shape: {val.shape}\" if isinstance(val, torch.Tensor) else val\n",
    "                     print(f\"  {attr}: {val_repr}\")\n",
    "        else:\n",
    "            print(\"\\nNo training data available to display sample.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not display sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37733f",
   "metadata": {},
   "source": [
    "## 6. Initialize and Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c481d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReactionTrainer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure dataset was loaded before proceeding\n",
    "if 'dataset' not in locals():\n",
    "     print(\"Error: Dataset not loaded. Please run the previous cell successfully.\")\n",
    "     # Stop execution\n",
    "     # raise RuntimeError(\"Dataset not available for trainer initialization.\")\n",
    "else:\n",
    "    # Additional keywords for trainer\n",
    "    additional_kwargs = {}\n",
    "    if params['finetune_lr'] is not None:\n",
    "        additional_kwargs['finetune_lr'] = params['finetune_lr']\n",
    "    if params['freeze_base_model']:\n",
    "        additional_kwargs['freeze_base_model'] = True\n",
    "    if params['eval_batch_size'] is not None:\n",
    "         additional_kwargs['eval_batch_size'] = params['eval_batch_size']\n",
    "\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = ReactionTrainer(\n",
    "        # Core training params from config\n",
    "        model_type=config.model.model_type,\n",
    "        readout=config.model.readout,\n",
    "        batch_size=config.training.batch_size,\n",
    "        max_epochs=config.training.max_epochs,\n",
    "        learning_rate=config.training.learning_rate,\n",
    "        output_dir=config.training.output_dir,\n",
    "        early_stopping_patience=config.training.early_stopping_patience,\n",
    "        save_best_model=config.training.save_best_model,\n",
    "        save_last_model=config.training.save_last_model,\n",
    "        random_seed=config.reaction.random_seed, # Use seed from reaction config for consistency\n",
    "        num_targets=len(config.reaction.target_fields),\n",
    "        use_scaler=config.reaction.use_scaler,\n",
    "        scalers=dataset.get_scalers(), # Get scalers from the loaded dataset\n",
    "        optimizer=config.training.optimizer,\n",
    "        weight_decay=config.training.weight_decay,\n",
    "        scheduler=config.training.scheduler,\n",
    "        warmup_epochs=config.training.warmup_epochs,\n",
    "        min_lr=config.training.min_lr,\n",
    "        gpu=config.training.gpu,\n",
    "        target_field_names=config.reaction.target_fields,\n",
    "        min_epochs=config.training.min_epochs,\n",
    "        num_workers=config.training.num_workers,\n",
    "        \n",
    "        # Model architecture params from config\n",
    "        node_dim=config.model.node_dim,\n",
    "        dropout=config.model.dropout,\n",
    "        use_layer_norm=config.model.use_layer_norm,\n",
    "        use_xtb_features=config.model.use_xtb_features,\n",
    "        num_xtb_features=config.model.num_xtb_features,\n",
    "        prediction_hidden_layers=config.model.prediction_hidden_layers,\n",
    "        prediction_hidden_dim=config.model.prediction_hidden_dim,\n",
    "        \n",
    "        # DimeNet++ specific params from config\n",
    "        hidden_channels=config.model.hidden_channels,\n",
    "        num_blocks=config.model.num_blocks,\n",
    "        cutoff=config.model.cutoff,\n",
    "        int_emb_size=config.model.int_emb_size,\n",
    "        basis_emb_size=config.model.basis_emb_size,\n",
    "        out_emb_channels=config.model.out_emb_channels,\n",
    "        num_spherical=config.model.num_spherical,\n",
    "        num_radial=config.model.num_radial,\n",
    "        envelope_exponent=config.model.envelope_exponent,\n",
    "        num_before_skip=config.model.num_before_skip,\n",
    "        num_after_skip=config.model.num_after_skip,\n",
    "        num_output_layers=config.model.num_output_layers,\n",
    "        max_num_neighbors=config.model.max_num_neighbors,\n",
    "        \n",
    "        # Pass additional kwargs like finetune_lr, freeze_base_model, etc.\n",
    "        **additional_kwargs \n",
    "    )\n",
    "    print(\"ReactionTrainer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5807425",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "*Note: This is where the actual training happens. If `epochs` is set low (e.g., 1), this will be very fast but won't result in a trained model. Increase `epochs` in Section 2 for real training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a25f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting continue training with 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/miniconda3/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                      | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0 | model          | MoleculePredictionModel   | 3.5 M  | train\n",
      "1 | net            | DimeNetPlusPlus           | 2.3 M  | train\n",
      "2 | readout_module | MeanReadout               | 0      | train\n",
      "3 | regr_or_cls_nn | MultiTargetPredictionHead | 1.2 M  | train\n",
      "---------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "13.866    Total estimated model params size (MB)\n",
      "193       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23231fdd0e849a3a047752f2ccf7502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9c97dad01421da2f0293a69b5e89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7c652659334b38aaab419d0711acc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_total_loss improved. New best score: 0.433\n",
      "Epoch 0, global step 80: 'val_total_loss' reached 0.43263 (best 0.43263), saving model to '/root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4326.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7386a0b1eb9d47929c30507164de7c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Test Avg MAE           4.679178714752197\n",
      "     Test Avg MAX_AE         17.82347869873047\n",
      "   Test Avg MEDIAN_AE        3.879878282546997\n",
      "      Test Avg MPAE          52.42013168334961\n",
      "       Test Avg R2           0.509222149848938\n",
      "      Test Avg RMSE          6.083659648895264\n",
      "      Test MAE DrG          4.0490851402282715\n",
      "     Test MAE G(TS)          5.309272289276123\n",
      "     Test MAX_AE DrG        19.114639282226562\n",
      "    Test MAX_AE G(TS)       16.532320022583008\n",
      "   Test MEDIAN_AE DrG        2.354483127593994\n",
      "  Test MEDIAN_AE G(TS)         5.4052734375\n",
      "      Test MPAE DrG          88.11637115478516\n",
      "     Test MPAE G(TS)        16.723892211914062\n",
      "       Test R2 DrG          0.7958351373672485\n",
      "      Test R2 G(TS)         0.22260910272598267\n",
      "      Test RMSE DrG          5.939459323883057\n",
      "     Test RMSE G(TS)        6.2278594970703125\n",
      "     test_total_loss        0.6442244052886963\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Training completed.\n",
      "Metrics: {'best_model_path': '/root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4326.ckpt', 'training_time': 71.42793154716492, 'epochs_completed': 1, 'mode': 'continue', 'test_metrics': {'test_total_loss': 0.6442244052886963, 'Test MAE G(TS)': 5.309272289276123, 'Test RMSE G(TS)': 6.2278594970703125, 'Test R2 G(TS)': 0.22260910272598267, 'Test MPAE G(TS)': 16.723892211914062, 'Test MAX_AE G(TS)': 16.532320022583008, 'Test MEDIAN_AE G(TS)': 5.4052734375, 'Test MAE DrG': 4.0490851402282715, 'Test RMSE DrG': 5.939459323883057, 'Test R2 DrG': 0.7958351373672485, 'Test MPAE DrG': 88.11637115478516, 'Test MAX_AE DrG': 19.114639282226562, 'Test MEDIAN_AE DrG': 2.354483127593994, 'Test Avg MAE': 4.679178714752197, 'Test Avg RMSE': 6.083659648895264, 'Test Avg R2': 0.509222149848938, 'Test Avg MPAE': 52.42013168334961, 'Test Avg MAX_AE': 17.82347869873047, 'Test Avg MEDIAN_AE': 3.879878282546997}}\n",
      "Best model saved to: /root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4326.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Ensure trainer and dataset are available\n",
    "if 'trainer' not in locals() or 'dataset' not in locals():\n",
    "    print(\"Error: Trainer or Dataset not initialized. Please run previous cells.\")\n",
    "    # Stop execution\n",
    "    # raise RuntimeError(\"Trainer or Dataset not available for fitting.\")\n",
    "elif params['cv_folds'] > 0:\n",
    "    print(\"Cross-validation is enabled. Training will be handled in the CV section (Section 10).\")\n",
    "    print(\"Skipping single training run.\")\n",
    "else:\n",
    "    # Start training (only if not doing cross-validation)\n",
    "    print(f\"Starting {params['mode']} training with {params['epochs']} epochs\")\n",
    "    # Make sure datasets are available\n",
    "    if dataset.train_data is None or dataset.val_data is None:\n",
    "         print(\"Error: Train or Validation data split not found. Check dataset loading and splitting.\")\n",
    "    else:\n",
    "        train_metrics = trainer.fit(\n",
    "            train_dataset=dataset.train_data,\n",
    "            val_dataset=dataset.val_data,\n",
    "            test_dataset=dataset.test_data, # Optional, used for final evaluation if provided\n",
    "            checkpoint_path=params['checkpoint_path'],\n",
    "            mode=params['mode']\n",
    "        )\n",
    "    \n",
    "        print(f\"Training completed.\")\n",
    "        print(\"Metrics:\", train_metrics)\n",
    "        if 'best_model_path' in train_metrics and train_metrics['best_model_path']:\n",
    "            print(f\"Best model saved to: {train_metrics['best_model_path']}\")\n",
    "        elif params['save_last_model'] and 'last_model_path' in train_metrics and train_metrics['last_model_path']:\n",
    "             print(f\"Last model saved to: {train_metrics['last_model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecbf95",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results\n",
    "\n",
    "*This section attempts to load the training logs saved by PyTorch Lightning and plot the loss and learning rate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1b2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics file not found at ./results/reaction_model/tensorboard/version_4/metrics.csv. Cannot plot results.\n",
      "Ensure training ran and generated logs.\n"
     ]
    }
   ],
   "source": [
    "# Display training metrics if training was run and logs exist\n",
    "# Check if trainer object exists and has completed fitting (indicated by presence of trainer.trainer)\n",
    "if 'trainer' in locals() and hasattr(trainer, 'trainer') and hasattr(trainer.trainer, 'logger'):\n",
    "    try:\n",
    "        # Construct the expected path to the CSV logs\n",
    "        # Default logger is CSVLogger, logs go into lightning_logs/version_X or a specified dir\n",
    "        log_dir = trainer.trainer.logger.log_dir\n",
    "        metrics_path = os.path.join(log_dir, 'metrics.csv')\n",
    "        \n",
    "        if os.path.exists(metrics_path):\n",
    "            print(f\"Loading metrics from: {metrics_path}\")\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            \n",
    "            # Fill NaN values that might occur if a metric wasn't logged every step/epoch\n",
    "            metrics_df = metrics_df.ffill().bfill()\n",
    "            \n",
    "            # Plot training and validation loss\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            train_loss_col = None\n",
    "            val_loss_col = None\n",
    "            # Find the correct column names (might vary slightly based on logging)\n",
    "            for col in metrics_df.columns:\n",
    "                if 'train_total_loss' in col and 'epoch' in col:\n",
    "                    train_loss_col = col\n",
    "                if 'val_total_loss' in col:\n",
    "                    val_loss_col = col\n",
    "\n",
    "            # Use 'epoch' column for x-axis if available, otherwise index\n",
    "            epoch_col = 'epoch' if 'epoch' in metrics_df.columns else metrics_df.index\n",
    "            \n",
    "            if train_loss_col:\n",
    "                 # Need to handle potential NaNs if logged per step vs per epoch\n",
    "                 train_loss_epoch = metrics_df.dropna(subset=[train_loss_col])\n",
    "                 epoch_axis_train = train_loss_epoch['epoch'] if 'epoch' in train_loss_epoch.columns else train_loss_epoch.index\n",
    "                 plt.plot(epoch_axis_train, train_loss_epoch[train_loss_col], label='Train Loss', marker='.')\n",
    "                 \n",
    "            if val_loss_col:\n",
    "                 # Val loss is typically logged per epoch, so NaNs are less likely unless training stopped early\n",
    "                 val_loss_epoch = metrics_df.dropna(subset=[val_loss_col])\n",
    "                 epoch_axis_val = val_loss_epoch['epoch'] if 'epoch' in val_loss_epoch.columns else val_loss_epoch.index\n",
    "                 plt.plot(epoch_axis_val, val_loss_epoch[val_loss_col], label='Val Loss', marker='.')\n",
    "\n",
    "            if train_loss_col or val_loss_col:\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Training and Validation Loss')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Could not find training/validation loss columns in metrics.csv\")\n",
    "                \n",
    "            # Plot learning rate\n",
    "            lr_col = None\n",
    "            # Find LR column (might depend on optimizer)\n",
    "            possible_lr_cols = [col for col in metrics_df.columns if col.startswith('lr-') or col == 'learning_rate']\n",
    "            if possible_lr_cols:\n",
    "                 lr_col = possible_lr_cols[0] # Take the first match\n",
    "            \n",
    "            if lr_col and lr_col in metrics_df.columns:\n",
    "                lr_epoch = metrics_df.dropna(subset=[lr_col])\n",
    "                epoch_axis_lr = lr_epoch['epoch'] if 'epoch' in lr_epoch.columns else lr_epoch.index\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.plot(epoch_axis_lr, lr_epoch[lr_col])\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('Learning Rate Schedule')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "            else:\n",
    "                 print(\"Could not find learning rate column in metrics.csv\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Metrics file not found at {metrics_path}. Cannot plot results.\")\n",
    "            print(\"Ensure training ran and generated logs.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Could not plot metrics: {e}\")\n",
    "        # traceback.print_exc()\n",
    "elif params['cv_folds'] > 0:\n",
    "     print(\"Plotting is skipped for cross-validation runs in this section.\")\n",
    "     print(\"Consider plotting results for each fold or aggregated results after CV finishes.\")\n",
    "else:\n",
    "    print(\"Trainer object not found or training hasn't run. Cannot plot results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640a791",
   "metadata": {},
   "source": [
    "## 9. Test Model Performance (Optional)\n",
    "\n",
    "*This evaluates the trained model (ideally the best one saved) on the held-out test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bcda10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4326.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /root/autodl-tmp/deepooooo19/results/reaction_model/checkpoints/best-epoch=0000-val_total_loss=0.4326.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac51a808381742e491812b0a11099c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Test Avg MAE           4.679178714752197\n",
      "     Test Avg MAX_AE         17.82347869873047\n",
      "   Test Avg MEDIAN_AE        3.879878282546997\n",
      "      Test Avg MPAE          52.42013168334961\n",
      "       Test Avg R2           0.509222149848938\n",
      "      Test Avg RMSE          6.083659648895264\n",
      "      Test MAE DrG          4.0490851402282715\n",
      "     Test MAE G(TS)          5.309272289276123\n",
      "     Test MAX_AE DrG        19.114639282226562\n",
      "    Test MAX_AE G(TS)       16.532320022583008\n",
      "   Test MEDIAN_AE DrG        2.354483127593994\n",
      "  Test MEDIAN_AE G(TS)         5.4052734375\n",
      "      Test MPAE DrG          88.11637115478516\n",
      "     Test MPAE G(TS)        16.723892211914062\n",
      "       Test R2 DrG          0.7958351373672485\n",
      "      Test R2 G(TS)         0.22260910272598267\n",
      "      Test RMSE DrG          5.939459323883057\n",
      "     Test RMSE G(TS)        6.2278594970703125\n",
      "     test_total_loss        0.6442244052886963\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Test Set Evaluation Results:\n",
      "  test_total_loss: 0.6442\n",
      "  Test MAE G(TS): 5.3093\n",
      "  Test RMSE G(TS): 6.2279\n",
      "  Test R2 G(TS): 0.2226\n",
      "  Test MPAE G(TS): 16.7239\n",
      "  Test MAX_AE G(TS): 16.5323\n",
      "  Test MEDIAN_AE G(TS): 5.4053\n",
      "  Test MAE DrG: 4.0491\n",
      "  Test RMSE DrG: 5.9395\n",
      "  Test R2 DrG: 0.7958\n",
      "  Test MPAE DrG: 88.1164\n",
      "  Test MAX_AE DrG: 19.1146\n",
      "  Test MEDIAN_AE DrG: 2.3545\n",
      "  Test Avg MAE: 4.6792\n",
      "  Test Avg RMSE: 6.0837\n",
      "  Test Avg R2: 0.5092\n",
      "  Test Avg MPAE: 52.4201\n",
      "  Test Avg MAX_AE: 17.8235\n",
      "  Test Avg MEDIAN_AE: 3.8799\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set if available and not doing CV\n",
    "if 'trainer' in locals() and hasattr(trainer, 'trainer') and 'dataset' in locals() and dataset.test_data and len(dataset.test_data) > 0 and params['cv_folds'] == 0:\n",
    "    try:\n",
    "        from torch_geometric.loader import DataLoader\n",
    "        \n",
    "        # Define which attributes need batching (specific to your data structure)\n",
    "        # These usually include atomic numbers (z) and positions (pos) for each state\n",
    "        follow_batch = []\n",
    "        sample_data = dataset.test_data[0]\n",
    "        if hasattr(sample_data, 'z0'): follow_batch.append('z0')\n",
    "        if hasattr(sample_data, 'pos0'): follow_batch.append('pos0')\n",
    "        if hasattr(sample_data, 'z1'): follow_batch.append('z1')\n",
    "        if hasattr(sample_data, 'pos1'): follow_batch.append('pos1')\n",
    "        if hasattr(sample_data, 'z2'): follow_batch.append('z2')\n",
    "        if hasattr(sample_data, 'pos2'): follow_batch.append('pos2')\n",
    "        # Add other batch-sensitive attributes if needed\n",
    "        \n",
    "        eval_batch_size = params['eval_batch_size'] if params['eval_batch_size'] is not None else params['batch_size']\n",
    "        test_loader = DataLoader(\n",
    "            dataset.test_data,\n",
    "            batch_size=eval_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=params['num_workers'],\n",
    "            follow_batch=follow_batch\n",
    "        )\n",
    "        \n",
    "        print(\"\\nEvaluating model on the test set...\")\n",
    "        # The trainer.test() method uses the best checkpoint by default if available\n",
    "        # Alternatively, you can specify a checkpoint_path='path/to/model.ckpt'\n",
    "        test_results = trainer.trainer.test(dataloaders=test_loader, ckpt_path='best') # Use 'best' or path to specific ckpt\n",
    "        \n",
    "        print(\"\\nTest Set Evaluation Results:\")\n",
    "        if test_results:\n",
    "             for key, value in test_results[0].items(): # Results are often in a list\n",
    "                 print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(\"No test results returned.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not evaluate on test set: Best checkpoint not found. Ensure training ran and saved a model.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Could not evaluate on test set: {e}\")\n",
    "        # traceback.print_exc()\n",
    "elif params['cv_folds'] > 0:\n",
    "     print(\"\\nTest set evaluation is typically done within or after the cross-validation loop (Section 10).\")\n",
    "elif 'dataset' in locals() and (dataset.test_data is None or len(dataset.test_data) == 0):\n",
    "    print(\"\\nNo test data available for evaluation.\")\n",
    "else:\n",
    "    print(\"\\nTrainer not available or not configured for single run. Skipping test set evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918142ca",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Training Example (Optional)\n",
    "\n",
    "If you want to perform cross-validation, use the following code:\n",
    "\n",
    "**Important:**\n",
    "* Make sure `params['cv_folds']` was set to a value greater than 0 in Section 2.\n",
    "* The `ReactionDataset` must be initialized with the same `cv_folds` value.\n",
    "* This cell will train the model multiple times (once for each fold), which can take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389d6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222efd93-53d8-4a20-bf3b-b444c75e6c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
