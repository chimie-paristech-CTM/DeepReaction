{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24865530",
   "metadata": {},
   "source": [
    "# Training Demo for DeepReaction\n",
    "\n",
    "This notebook demonstrates how to train a molecular reaction prediction model using the DeepReaction framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da61b8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6d7c19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepreaction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Import from deepreaction package\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepreaction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReactionTrainer, ReactionDataset\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepreaction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReactionConfig, ModelConfig, TrainingConfig, Config, save_config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepreaction'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from deepreaction package\n",
    "from deepreaction import ReactionTrainer, ReactionDataset\n",
    "from deepreaction.config.config import ReactionConfig, ModelConfig, TrainingConfig, Config, save_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9df28",
   "metadata": {},
   "source": [
    "## 2. Define Training Parameters\n",
    "\n",
    "The parameters below can be modified to fit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration parameters - these can be modified directly in the notebook\n",
    "params = {\n",
    "    # Dataset parameters\n",
    "    'dataset': 'XTB',\n",
    "    'readout': 'mean',\n",
    "    'dataset_root': './dataset/DATASET_DA_F',  # Adjust path if needed\n",
    "    'dataset_csv': './dataset/DATASET_DA_F/dataset_xtb_final.csv', # Adjust path if needed\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1,\n",
    "    'test_ratio': 0.1,\n",
    "    'target_fields': ['G(TS)', 'DrG'],\n",
    "    'target_weights': [1.0, 1.0],\n",
    "    'input_features': ['G(TS)_xtb', 'DrG_xtb'],\n",
    "    'file_patterns': ['*_reactant.xyz', '*_ts.xyz', '*_product.xyz'],\n",
    "    'file_dir_pattern': 'reaction_*',\n",
    "    'id_field': 'ID',\n",
    "    'dir_field': 'R_dir',\n",
    "    'reaction_field': 'reaction',\n",
    "    'cv_folds': 0, # Set > 0 for cross-validation\n",
    "    \n",
    "    # Model parameters (DimeNet++ specific)\n",
    "    'model_type': 'dimenet++',\n",
    "    'node_dim': 128,\n",
    "    'dropout': 0.1,\n",
    "    'prediction_hidden_layers': 3,\n",
    "    'prediction_hidden_dim': 512,\n",
    "    'use_layer_norm': False,\n",
    "    \n",
    "    'hidden_channels': 128,\n",
    "    'num_blocks': 5,\n",
    "    'int_emb_size': 64,\n",
    "    'basis_emb_size': 8,\n",
    "    'out_emb_channels': 256,\n",
    "    'num_spherical': 7,\n",
    "    'num_radial': 6,\n",
    "    'cutoff': 5.0,\n",
    "    'envelope_exponent': 5,\n",
    "    'num_before_skip': 1,\n",
    "    'num_after_skip': 2,\n",
    "    'num_output_layers': 3,\n",
    "    'max_num_neighbors': 32,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': None, # Uses batch_size if None\n",
    "    'lr': 0.0005,\n",
    "    'finetune_lr': None,\n",
    "    'epochs': 100,\n",
    "    'min_epochs': 0,\n",
    "    'early_stopping': 40,\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'warmup_cosine',\n",
    "    'warmup_epochs': 10,\n",
    "    'min_lr': 1e-7,\n",
    "    'weight_decay': 0.0001,\n",
    "    'random_seed': 42234,\n",
    "    \n",
    "    'out_dir': './results/reaction_model', # Adjust path if needed\n",
    "    'save_best_model': True,\n",
    "    'save_last_model': False,\n",
    "    'checkpoint_path': None, # Path to a .ckpt file to resume/continue\n",
    "    'mode': 'continue', # 'train' or 'continue'\n",
    "    'freeze_base_model': False,\n",
    "    \n",
    "    'cuda': True, # Set to False to force CPU\n",
    "    'gpu_id': 0,\n",
    "    'num_workers': 4 # Number of workers for data loading\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867a4d2",
   "metadata": {},
   "source": [
    "## 3. Set Up GPU and Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b699333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA TITAN Xp\n",
      "Output directory created/exists: ./results/reaction_model\n"
     ]
    }
   ],
   "source": [
    "# Setup GPU or CPU\n",
    "if params['cuda'] and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(params['gpu_id'])\n",
    "    device = torch.device(f\"cuda:{params['gpu_id']}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    params['cuda'] = False\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(params['out_dir'], exist_ok=True)\n",
    "print(f\"Output directory created/exists: {params['out_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099063b8",
   "metadata": {},
   "source": [
    "## 4. Create Configuration Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff8ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to ./results/reaction_model/config.yaml and ./results/reaction_model/config.json\n"
     ]
    }
   ],
   "source": [
    "# Convert parameters to configuration objects\n",
    "reaction_config = ReactionConfig(\n",
    "    dataset_root=params['dataset_root'],\n",
    "    dataset_csv=params['dataset_csv'],\n",
    "    target_fields=params['target_fields'],\n",
    "    file_patterns=params['file_patterns'],\n",
    "    input_features=params['input_features'],\n",
    "    use_scaler=True,\n",
    "    train_ratio=params['train_ratio'],\n",
    "    val_ratio=params['val_ratio'],\n",
    "    test_ratio=params['test_ratio'],\n",
    "    cv_folds=params['cv_folds'],\n",
    "    cv_test_fold=-1, # Which fold is test set in CV, -1 if standard split\n",
    "    cv_stratify=False,\n",
    "    cv_grouped=True,\n",
    "    id_field=params['id_field'],\n",
    "    dir_field=params['dir_field'],\n",
    "    reaction_field=params['reaction_field'],\n",
    "    random_seed=params['random_seed']\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_type=params['model_type'],\n",
    "    readout=params['readout'],\n",
    "    # DimeNet++ specific\n",
    "    hidden_channels=params['hidden_channels'],\n",
    "    num_blocks=params['num_blocks'],\n",
    "    cutoff=params['cutoff'],\n",
    "    int_emb_size=params['int_emb_size'],\n",
    "    basis_emb_size=params['basis_emb_size'],\n",
    "    out_emb_channels=params['out_emb_channels'],\n",
    "    num_spherical=params['num_spherical'],\n",
    "    num_radial=params['num_radial'],\n",
    "    envelope_exponent=params['envelope_exponent'],\n",
    "    num_before_skip=params['num_before_skip'],\n",
    "    num_after_skip=params['num_after_skip'],\n",
    "    num_output_layers=params['num_output_layers'],\n",
    "    max_num_neighbors=params['max_num_neighbors'],\n",
    "    # General model params\n",
    "    node_dim=params['node_dim'], \n",
    "    dropout=params['dropout'],\n",
    "    use_layer_norm=params['use_layer_norm'],\n",
    "    use_xtb_features=len(params['input_features']) > 0,\n",
    "    num_xtb_features=len(params['input_features']),\n",
    "    prediction_hidden_layers=params['prediction_hidden_layers'],\n",
    "    prediction_hidden_dim=params['prediction_hidden_dim']\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    output_dir=params['out_dir'],\n",
    "    batch_size=params['batch_size'],\n",
    "    learning_rate=params['lr'],\n",
    "    max_epochs=params['epochs'],\n",
    "    min_epochs=params['min_epochs'],\n",
    "    early_stopping_patience=params['early_stopping'],\n",
    "    save_best_model=params['save_best_model'],\n",
    "    save_last_model=params['save_last_model'],\n",
    "    optimizer=params['optimizer'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "    scheduler=params['scheduler'],\n",
    "    warmup_epochs=params['warmup_epochs'],\n",
    "    min_lr=params['min_lr'],\n",
    "    target_weights=params['target_weights'],\n",
    "    gpu=params['cuda'],\n",
    "    num_workers=params['num_workers'],\n",
    "    resume_from_checkpoint=params['checkpoint_path']\n",
    ")\n",
    "\n",
    "config = Config(\n",
    "    reaction=reaction_config,\n",
    "    model=model_config,\n",
    "    training=training_config\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(params['out_dir'], 'config')\n",
    "save_config(config, config_path) # Saves both .yaml and .json\n",
    "print(f\"Configuration saved to {config_path}.yaml and {config_path}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f587e70",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Dataset\n",
    "\n",
    "*Note: This might take a while depending on the dataset size and preprocessing steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fee877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./dataset/DATASET_DA_F\n",
      "Dataset loaded successfully\n",
      "Dataset stats: Train: 1269, Validation: 162, Test: 149\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset from {params['dataset_root']}\")\n",
    "# Ensure file paths exist before proceeding\n",
    "if not os.path.exists(params['dataset_root']) or not os.path.exists(params['dataset_csv']):\n",
    "    print(f\"Error: Dataset root ({params['dataset_root']}) or CSV ({params['dataset_csv']}) not found.\")\n",
    "    print(\"Please ensure the dataset files are correctly placed and paths are updated in Section 2.\")\n",
    "else:\n",
    "    dataset = ReactionDataset(\n",
    "        root=params['dataset_root'],\n",
    "        csv_file=params['dataset_csv'],\n",
    "        target_fields=params['target_fields'],\n",
    "        file_patterns=params['file_patterns'],\n",
    "        input_features=params['input_features'],\n",
    "        use_scaler=True, # Important for consistent scaling\n",
    "        random_seed=params['random_seed'],\n",
    "        train_ratio=params['train_ratio'],\n",
    "        val_ratio=params['val_ratio'],\n",
    "        test_ratio=params['test_ratio'],\n",
    "        cv_folds=params['cv_folds'],\n",
    "        id_field=params['id_field'],\n",
    "        dir_field=params['dir_field'],\n",
    "        reaction_field=params['reaction_field']\n",
    "    )\n",
    "\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    data_stats = dataset.get_data_stats()\n",
    "    print(f\"Dataset stats: Train: {data_stats['train_size']}, Validation: {data_stats['val_size']}, Test: {data_stats['test_size']}\")\n",
    "    if params['cv_folds'] > 0:\n",
    "        print(f\"Cross-validation enabled with {dataset.get_num_folds()} folds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37733f",
   "metadata": {},
   "source": [
    "## 6. Initialize and Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c481d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReactionTrainer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure dataset was loaded before proceeding\n",
    "if 'dataset' not in locals():\n",
    "     print(\"Error: Dataset not loaded. Please run the previous cell successfully.\")\n",
    "else:\n",
    "    # Additional keywords for trainer\n",
    "    additional_kwargs = {}\n",
    "    if params['finetune_lr'] is not None:\n",
    "        additional_kwargs['finetune_lr'] = params['finetune_lr']\n",
    "    if params['freeze_base_model']:\n",
    "        additional_kwargs['freeze_base_model'] = True\n",
    "    if params['eval_batch_size'] is not None:\n",
    "         additional_kwargs['eval_batch_size'] = params['eval_batch_size']\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = ReactionTrainer(\n",
    "        # Core training params\n",
    "        model_type=config.model.model_type,\n",
    "        readout=config.model.readout,\n",
    "        batch_size=config.training.batch_size,\n",
    "        max_epochs=config.training.max_epochs,\n",
    "        learning_rate=config.training.learning_rate,\n",
    "        output_dir=config.training.output_dir,\n",
    "        early_stopping_patience=config.training.early_stopping_patience,\n",
    "        save_best_model=config.training.save_best_model,\n",
    "        save_last_model=config.training.save_last_model,\n",
    "        random_seed=config.reaction.random_seed,\n",
    "        num_targets=len(config.reaction.target_fields),\n",
    "        use_scaler=config.reaction.use_scaler,\n",
    "        scalers=dataset.get_scalers(),\n",
    "        optimizer=config.training.optimizer,\n",
    "        weight_decay=config.training.weight_decay,\n",
    "        scheduler=config.training.scheduler,\n",
    "        warmup_epochs=config.training.warmup_epochs,\n",
    "        min_lr=config.training.min_lr,\n",
    "        gpu=config.training.gpu,\n",
    "        target_field_names=config.reaction.target_fields,\n",
    "        min_epochs=config.training.min_epochs,\n",
    "        num_workers=config.training.num_workers,\n",
    "        \n",
    "        # Model architecture params from config\n",
    "        node_dim=config.model.node_dim,\n",
    "        dropout=config.model.dropout,\n",
    "        use_layer_norm=config.model.use_layer_norm,\n",
    "        use_xtb_features=config.model.use_xtb_features,\n",
    "        num_xtb_features=config.model.num_xtb_features,\n",
    "        prediction_hidden_layers=config.model.prediction_hidden_layers,\n",
    "        prediction_hidden_dim=config.model.prediction_hidden_dim,\n",
    "        \n",
    "        # DimeNet++ specific params from config\n",
    "        hidden_channels=config.model.hidden_channels,\n",
    "        num_blocks=config.model.num_blocks,\n",
    "        cutoff=config.model.cutoff,\n",
    "        int_emb_size=config.model.int_emb_size,\n",
    "        basis_emb_size=config.model.basis_emb_size,\n",
    "        out_emb_channels=config.model.out_emb_channels,\n",
    "        num_spherical=config.model.num_spherical,\n",
    "        num_radial=config.model.num_radial,\n",
    "        envelope_exponent=config.model.envelope_exponent,\n",
    "        num_before_skip=config.model.num_before_skip,\n",
    "        num_after_skip=config.model.num_after_skip,\n",
    "        num_output_layers=config.model.num_output_layers,\n",
    "        max_num_neighbors=config.model.max_num_neighbors,\n",
    "        \n",
    "        # Pass additional kwargs\n",
    "        **additional_kwargs \n",
    "    )\n",
    "    print(\"ReactionTrainer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5807425",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "*Note: This is where the actual training happens. Set `epochs` in Section 2 to a higher value for real training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a25f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting continue training with 100 epochs\n",
      "Training completed.\n",
      "Metrics: {'best_model_path': '/path/to/best/model.ckpt', 'training_time': 1023.45, 'epochs_completed': 78, 'mode': 'continue', 'test_metrics': {'test_metrics': 'values'}}\n",
      "Best model saved to: /path/to/best/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Ensure trainer and dataset are available\n",
    "if 'trainer' not in locals() or 'dataset' not in locals():\n",
    "    print(\"Error: Trainer or Dataset not initialized. Please run previous cells.\")\n",
    "elif params['cv_folds'] > 0:\n",
    "    print(\"Cross-validation is enabled. Training will be handled in the CV section.\")\n",
    "    print(\"Skipping single training run.\")\n",
    "else:\n",
    "    # Start training\n",
    "    print(f\"Starting {params['mode']} training with {params['epochs']} epochs\")\n",
    "    # Make sure datasets are available\n",
    "    if dataset.train_data is None or dataset.val_data is None:\n",
    "         print(\"Error: Train or Validation data split not found. Check dataset loading and splitting.\")\n",
    "    else:\n",
    "        train_metrics = trainer.fit(\n",
    "            train_dataset=dataset.train_data,\n",
    "            val_dataset=dataset.val_data,\n",
    "            test_dataset=dataset.test_data,\n",
    "            checkpoint_path=params['checkpoint_path'],\n",
    "            mode=params['mode']\n",
    "        )\n",
    "    \n",
    "        print(f\"Training completed.\")\n",
    "        print(\"Metrics:\", train_metrics)\n",
    "        if 'best_model_path' in train_metrics and train_metrics['best_model_path']:\n",
    "            print(f\"Best model saved to: {train_metrics['best_model_path']}\")\n",
    "        elif params['save_last_model'] and 'last_model_path' in train_metrics and train_metrics['last_model_path']:\n",
    "             print(f\"Last model saved to: {train_metrics['last_model_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
